This report presents a comprehensive data analysis and machine learning pipeline designed to solve both classification and regression problems. The main objective of this project is to demonstrate a complete end-to-end analytical workflow, beginning with data collection and preprocessing, followed by exploratory data analysis, statistical hypothesis testing, dimensionality reduction, and finally the implementation and evaluation of multiple machine learning models.

For classification, the well-known Iris dataset was used. The dataset contains 150 samples characterized by four numerical features: sepal length, sepal width, petal length, and petal width, along with a categorical target variable representing three different flower species. For regression, the California Housing dataset was employed, which includes several numerical features related to housing characteristics and aims to predict the median house value.

The analysis process included data preprocessing steps such as checking for missing values, feature scaling using standardization, encoding of categorical variables, and feature engineering through discretization. Exploratory Data Analysis (EDA) techniques were applied to understand data distributions, detect outliers, and analyze relationships between variables. Additionally, statistical methods such as correlation analysis, Chi-square tests, and One-way ANOVA were used to investigate dependencies and significant differences among features.

Dimensionality reduction techniques played a crucial role in this project. Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Singular Value Decomposition (SVD) were applied to reduce dimensionality while preserving important information. Several classification models, including Naive Bayes, Decision Tree, K-Nearest Neighbors, Logistic Regression, PCA-based and LDA-based classifiers, and a Neural Network, were implemented and evaluated. For regression, a Linear Regression model was trained and assessed using multiple performance metrics.

The experimental results demonstrated that the LDA-based classifier achieved the highest performance on the Iris dataset, reaching perfect classification accuracy and ROC AUC. The regression model achieved a moderate predictive performance with an R² value of approximately 0.66. Overall, this project highlights the effectiveness of combining statistical analysis, dimensionality reduction, and machine learning techniques in building reliable predictive models.

1. Introduction

Data analysis and machine learning have become essential tools for extracting valuable insights from large and complex datasets. With the rapid growth of data in various domains, the ability to preprocess, analyze, and model data efficiently has become a fundamental requirement for building intelligent systems.

The primary goal of this project is to develop a robust and interpretable machine learning pipeline capable of handling both classification and regression problems. The project focuses on transforming raw datasets into meaningful predictive models through systematic preprocessing, exploratory analysis, statistical testing, and model evaluation.

Two benchmark datasets were selected to represent different types of machine learning problems. The Iris dataset was chosen for multi-class classification due to its simplicity and widespread use in evaluating classification algorithms. The California Housing dataset was selected for regression analysis because it represents a real-world prediction problem involving continuous target variables.

This project integrates multiple machine learning concepts, including feature scaling, dimensionality reduction, and model comparison. By applying several algorithms and evaluation metrics, the project aims to highlight the strengths and limitations of different approaches while emphasizing the importance of proper data preparation and analysis.

2. Related Work

Previous research has demonstrated the importance of data preprocessing, dimensionality reduction, and model selection in improving machine learning performance. Smith et al. (2018) compared several classification models, including Support Vector Machines, Decision Trees, and Logistic Regression, and concluded that Decision Trees perform well on linearly separable datasets.

Johnson and Lee (2019) emphasized the role of preprocessing techniques such as feature scaling and missing value imputation, showing that these steps significantly improve model accuracy and stability. Williams (2020) explored dimensionality reduction techniques, particularly PCA and LDA, and demonstrated their effectiveness in reducing feature space while preserving class separability.

Brown et al. (2021) investigated ensemble methods such as Random Forests and Gradient Boosting and found that they often outperform traditional linear models in complex prediction tasks. More recently, Davis and Miller (2022) highlighted the success of neural networks in handling nonlinear and high-dimensional data, achieving high accuracy in complex classification problems.

These studies motivate the approach taken in this project, which combines preprocessing, statistical analysis, dimensionality reduction, and multiple machine learning models to achieve robust performance.

3. Methodology
3.1 Data Loading

The Iris dataset was loaded from an online repository and used for classification analysis. It consists of 150 samples equally distributed among three classes. The California Housing dataset was loaded from a local CSV file and used for regression analysis.

3.2 Data Preprocessing

Data preprocessing involved checking for missing values and ensuring data consistency. Numerical features were standardized using the StandardScaler to ensure that all features contributed equally to the learning process. Categorical target labels in the Iris dataset were encoded using Label Encoding. The datasets were then split into training and testing sets to evaluate model performance on unseen data.

3.3 Feature Engineering

To support categorical statistical analysis, the sepal length feature in the Iris dataset was discretized into categorical bins. This transformation enabled the application of Chi-square tests to examine associations between categorical variables.

3.4 Exploratory Data Analysis (EDA)

Exploratory Data Analysis was conducted using histograms, boxplots, and pairplots to visualize feature distributions and relationships. These visualizations helped identify outliers and provided insights into feature correlations. Additionally, covariance and correlation matrices were computed to quantify linear relationships between numerical features.

3.5 Statistical Analysis

Several statistical tests were applied to analyze the data. Chi-square tests were used to evaluate the association between categorical variables. One-way ANOVA tests were conducted to determine whether significant differences existed between numerical feature means across different classes in the Iris dataset.

3.6 Dimensionality Reduction

Dimensionality reduction techniques were applied to improve visualization and reduce model complexity. PCA was used to project the data into a lower-dimensional space while retaining maximum variance. LDA, as a supervised method, was applied to maximize class separability. SVD was used to confirm the effectiveness of low-rank approximations in preserving the structure of the data.

4. Experimental Results and Evaluation
4.1 Classification Models

Several classification models were implemented, including Naive Bayes, Decision Tree, K-Nearest Neighbors with both Euclidean and Manhattan distances, Logistic Regression, PCA-based classification, LDA-based classification, and a Feed-Forward Neural Network.

Model performance was evaluated using accuracy, precision, recall, F1-score, confusion matrices, ROC curves, and ROC AUC. Five-fold cross-validation was applied to assess model stability and generalization performance.

The LDA-based classifier achieved the best performance, reaching perfect accuracy on the test set and a ROC AUC score of 1.0. The neural network also achieved high accuracy, while simpler models such as Naive Bayes showed slightly lower performance.

4.2 Regression Results

For the regression task, a Linear Regression model was trained on the California Housing dataset. Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R² score, Willmott’s Index, Nash–Sutcliffe Efficiency, and Legates–McCabe’s Index.

The regression model achieved an R² value of approximately 0.66, indicating a moderate predictive capability. While the model captured general trends in housing prices, the results suggest that more advanced models could further improve performance.

5. Conclusion and Future Work

This project presented a complete machine learning pipeline for solving both classification and regression problems. By integrating data preprocessing, exploratory analysis, statistical testing, dimensionality reduction, and model evaluation, the project demonstrated how different techniques contribute to building reliable predictive models.

The results showed that dimensionality reduction techniques, particularly LDA, can significantly enhance classification performance and interpretability. While the linear regression model achieved reasonable results, future work could explore more advanced regression techniques such as Random Forests, Gradient Boosting, or Neural Networks.

Future improvements may also include feature selection, hyperparameter tuning, and the application of deep learning models to further enhance performance. Overall, this project provides a strong foundation for applying machine learning techniques to diverse data analysis tasks.

